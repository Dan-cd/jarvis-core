from Jarvis.core.decision import DecisionOutcome, DecisionPath
from Jarvis.core.action_request import ActionRequest
from Jarvis.core.action_result import ActionResult
from Jarvis.core.errors import (
    WebRequiredButUnavailable,
    InvalidAnswerOrigin,
    InvalidActionResult
)


class Executor:
    """
    Executor principal do Jarvis.
    - Executa Decisions vindas do Router.
    - Suporta execu√ß√£o de LLM, Plugins (incluindo RAG) e rotas locais.
    - Valida contratos de ActionResult antes de enviar ao AnswerPipeline.
    """

    def __init__(
        self,
        llm,
        fallback,
        sandbox,
        memory,
        execution_memory,
        temp_memory,
        context,
        answer_pipeline,
        dev_guard
    ):
        self.llm = llm
        self.fallback = fallback
        self.sandbox = sandbox
        self.memory = memory
        self.execution_memory = execution_memory
        self.temp_memory = temp_memory
        self.context = context
        self.answer_pipeline = answer_pipeline
        self.dev_guard = dev_guard

    def execute(self, decision, user_input: str) -> str:
        """
        Executa uma Decision e retorna a resposta formatada pelo AnswerPipeline.
        """

        # Limpamos a mem√≥ria de execu√ß√£o se ela existir (inje√ß√£o pelo Context).
        if self.execution_memory:
            try:
                self.execution_memory.clear()
            except Exception:
                # N√£o deixamos falhas de limpeza interromperem o fluxo principal.
                pass

        # ----- decis√µes finais / curta-circuito -----
        if decision.path is None:
            # Decision final: usar reason/message como resposta ou erro institucional
            if decision.outcome == DecisionOutcome.DENY:
                return self.answer_pipeline.system_error(getattr(decision, "reason", "") or "")
            if decision.outcome == DecisionOutcome.OFFLINE:
                return self.answer_pipeline.system_error(getattr(decision, "reason", "") or "")
            if decision.outcome == DecisionOutcome.REQUIRE_DEV_MODE:
                # Intercepta solicita√ß√£o de dev mode para autentica√ß√£o
                msg = getattr(decision, "reason", "") or "Requer modo desenvolvedor."
                return self._handle_dev_auth_challenge(msg)
            # fallback: devolver reason como resposta local
            return self.answer_pipeline.build(
                response=getattr(decision, "reason", "") or "",
                origin="local",
                confidence=1.0
            )

        # ----- roteamento por path -----
        if decision.path == DecisionPath.LLM:
            return self._execute_llm(decision, user_input)

        if decision.path == DecisionPath.PLUGIN:
            return self._execute_plugin(decision, user_input)

        if decision.path == DecisionPath.LOCAL:
            return self._execute_local(decision)

        # Se chegou aqui, caminho inv√°lido
        raise InvalidAnswerOrigin(f"Caminho de execu√ß√£o inv√°lido: {decision.path}")

    # -------------------------
    # Execu√ß√£o LLM (simples)
    # -------------------------
    def _execute_llm(self, decision, user_input: str) -> str:
        """
        Chamada direta ao LLM (chat/help/unknown).
        O LLMManager j√° injeta o system prompt; aqui passamos o prompt do usu√°rio.
        """

        # Marca origem no ciclo
        if self.execution_memory:
            self.execution_memory.set("origin", "llm")

        # Gerar texto. O LLMManager aplicar√° o system prompt internamente.
        response_text = self.llm.generate(prompt=user_input, mode=decision.payload.get("mode", "default"))

        result = ActionResult(
            content=response_text,
            origin="llm",
            confidence=0.65
        )

        # Validar contrato (origem, content, confidence)
        self._validate_action_result(result, expected_origin="llm")

        return self.answer_pipeline.build_from_result(result)

    # -------------------------
    # Execu√ß√£o de Plugin (inclui RAG)
    # -------------------------
    def _execute_plugin(self, decision, user_input: str) -> str:
        """
        Executa plugin. Se payload.temporal == True, aplica RAG (Web -> LLM).
        Caso contr√°rio, retorna resultado do plugin diretamente.
        """

        intent = decision.payload.get("intent")
        plugins = decision.payload.get("plugins", []) or []
        is_temporal = bool(decision.payload.get("temporal"))

        if not plugins:
            raise WebRequiredButUnavailable("Nenhum plugin dispon√≠vel para a inten√ß√£o.")

        # Instancia plugin se necess√°rio
        plugin_ref = plugins[0]
        plugin = plugin_ref() if isinstance(plugin_ref, type) else plugin_ref

        # Prepara ActionRequest
        params = getattr(intent, "payload", None) or {"query": intent.raw}
        action = ActionRequest(intent=intent, params=params, context=self.context)

        # Executa plugin
        raw_result = plugin.execute(action)

        if not isinstance(raw_result, ActionResult):
            raise InvalidActionResult(f"Plugin {getattr(plugin, 'name', '?')} retornou tipo inv√°lido.")

        # Se consulta temporal: sintetizar com LLM (RAG)
        if is_temporal:
            return self._synthesize_web_with_llm(user_query=user_input, web_result=raw_result)

        # Validate as response from plugin (web/plugin)
        expected_origin = "web" if getattr(raw_result, "origin", None) == "web" else "plugin"
        self._validate_action_result(raw_result, expected_origin=expected_origin)

        return self.answer_pipeline.build_from_result(raw_result)

    # -------------------------
    # Execu√ß√£o local (mem√≥ria/util)
    # -------------------------
    def _execute_local(self, decision) -> str:
        if self.execution_memory:
            self.execution_memory.set("origin", "local")

        # Espera-se que memory.execute retorne string adequada
        content = self.memory.execute(decision.payload)

        result = ActionResult(content=content, origin="local", confidence=0.9)
        self._validate_action_result(result, expected_origin="local")
        return self.answer_pipeline.build_from_result(result)

    # -------------------------
    # RAG: sintetizar web result com LLM
    # -------------------------
    def _synthesize_web_with_llm(self, user_query: str, web_result: ActionResult) -> str:
        """
        Monta um prompt composto (user_query + web_result + fontes) e chama o LLM.
        A responsabilidade de garantir voz institucional est√° no LLMManager (system prompt).
        """

        # Assegura que web_result.content √© string
        web_content = web_result.content if isinstance(web_result.content, str) else str(web_result.content)

        prompt_parts = [
            f"Pergunta: {user_query}",
            "",
            "Resultado da web (fontes abaixo):",
            web_content,
            ""
        ]

        sources_list = None
        data = getattr(web_result, "data", None)
        if isinstance(data, dict):
            sources_list = data.get("sources")
        elif hasattr(data, "sources"):
            sources_list = getattr(data, "sources")

        if sources_list:
            prompt_parts.append("Fontes:")
            for s in sources_list:
                prompt_parts.append(f"- {s}")
            prompt_parts.append("")

        prompt_parts.append("Instru√ß√µes de S√≠ntese:")
        prompt_parts.append("1. Responda APENAS com base nos resultados acima.")
        prompt_parts.append("2. Se os resultados contiverem a resposta, sintetize-a em portugu√™s.")
        prompt_parts.append("3. Se os resultados estiverem vazios ou irrelevantes, diga: 'N√£o encontrei informa√ß√µes suficientes nos resultados da busca.'")
        prompt_parts.append("4. N√ÉO sugira que o usu√°rio pesquise novamente; voc√™ j√° pesquisou.")
        prompt_parts.append("Com base nisso, resuma e responda de forma clara e pr√°tica:")

        rag_prompt = "\n".join(prompt_parts)

        # Marca origem (ciclo)
        if self.execution_memory:
            self.execution_memory.set("origin", "llm")

        # Chama LLM com mode "rag" (LLMManager ir√° aplicar system prompt)
        response_text = self.llm.generate(prompt=rag_prompt, mode="rag")

        # Monta ActionResult sintetizado
        result = ActionResult(content=response_text, origin="llm", confidence=getattr(web_result, "confidence", 0.6) or 0.6)

        # Anexa fontes para AnswerPipeline exibir
        result.data = {"sources": sources_list or []}

        # Valida e retorna
        self._validate_action_result(result, expected_origin="llm")
        return self.answer_pipeline.build_from_result(result)

    # -------------------------
    # Valida√ß√£o de contrato
    # -------------------------
    def _validate_action_result(self, result: ActionResult, expected_origin: str):
        if not isinstance(result, ActionResult):
            raise InvalidActionResult("Resultado n√£o √© ActionResult.")

        if getattr(result, "origin", None) != expected_origin:
            raise InvalidAnswerOrigin(f"Origem inv√°lida: esperado={expected_origin}, recebido={getattr(result, 'origin', None)}")

        if not isinstance(result.content, str):
            raise InvalidActionResult("content precisa ser string.")

        if not isinstance(result.confidence, (int, float)):
            raise InvalidActionResult("confidence inv√°lida.")

        if not 0 <= result.confidence <= 1:
            raise InvalidActionResult("confidence fora do intervalo 0‚Äì1.")

    # -------------------------
    # Autentica√ß√£o DEV
    # -------------------------
    def _handle_dev_auth_challenge(self, reason: str) -> str:
        """
        Solicita senha ao usu√°rio se a interface permitir (CLI input).
        Como o Executor roda num passo s√≠ncrono, usamos input() direto aqui
        para manter a simplicidade do fluxo (conforme v6.5), 
        embora idealmente isso fosse ass√≠ncrono.
        """
        print(f"\nüîí {reason}")
        print("Digite a senha de administrador (ou Enter para cancelar):")
        try:
            password = input("Senha> ").strip()
        except EOFError:
            return self.answer_pipeline.system_error("Entrada cancelada.")
        
        if not password:
            return self.answer_pipeline.system_error("Autentica√ß√£o cancelada.")

        # Importa√ß√£o tardia para evitar ciclo ou garantir contexto
        from Jarvis.core.dev_mode import DevModeManager
        # Config j√° est√° no context? N√£o, precisamos passar config.
        # O Executor recebe context, mas DevModeManager precisa de config.
        # Vamos tentar pegar do bootstrap ou assumir que est√° no context.
        # Ajuste: DevModeManager(context, config)
        
        # HACK: Se context n√£o tiver config, pegamos do singleton ou erro
        config = getattr(self.context, "config", None)
        if not config:
             # Tenta instanciar nova config (cacheada)
             from Jarvis.core.config import Config
             config = Config()

        manager = DevModeManager(self.context, config)
        result = manager.enter(password)
        
        if self.context.dev_mode:
            return f"‚úÖ {result}"
        else:
            return f"‚ùå {result}"
